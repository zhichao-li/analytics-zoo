/* DO NOT EDIT THIS FILE - it is machine generated */
#include <cstring>
#include <stdio.h>
#include <stdlib.h>
#include <cstdio>
#include <cstdint>
#include <cstdlib>
#include <cassert>
#include <stdexcept>
#include <sstream>
#include <string>
#include <iostream>
#include <torch/script.h> // One-stop header.
#include <iostream>
#include <memory>


#include "com_intel_analytics_zoo_pytorch_PytorchModel.h"
using namespace std;

extern "C" {


std::shared_ptr<torch::jit::script::Module> model_ptr = nullptr

/*
 * Class:     com_intel_analytics_zoo_pytorch_PytorchModel
 * Method:    load
 * Signature: (Ljava/lang/String;)V
 */
JNIEXPORT void JNICALL Java_com_intel_analytics_zoo_pytorch_PytorchModel_load
  (JNIEnv *jenv, jobject jobj, jstring jmodel_path) {

    const char* p_model_path = jenv->GetStringUTFChars(jmodel_path, NULL);

    // Deserialize the ScriptModule from a file using torch::jit::load().
    model_ptr = torch::jit::load(p_model_path);
    assert(model_ptr != nullptr);
    std::cout << "Model loaded ok\n";
    // release
    jenv->ReleaseStringUTFChars(jmodel_path, p_model_path);
  }



/*
 * Class:     com_intel_analytics_zoo_pytorch_PytorchModel
 * Method:    forward
 * Signature: ([F[I)Lcom/intel/analytics/zoo/pipeline/inference/JTensor;
 */
JNIEXPORT jobject JNICALL Java_com_intel_analytics_zoo_pytorch_PytorchModel_forward
  (JNIEnv * jenv, jobject jobj, jfloatArray jstorage, jint joffset, jintArray jshape) {
    
//    auto model_ptr = torch::jit::load("/opt/work/model.pt");
 //   assert(model_ptr != nullptr);
   // std::cout << "Model loaded ok\n";

    jfloat* c_storage = (jfloat*) jenv -> GetPrimitiveArrayCritical(jstorage, JNI_FALSE);
    int c_storage_len = jenv -> GetArrayLength(jstorage);
    jint* c_shape = (jint*) jenv -> GetPrimitiveArrayCritical(jshape, JNI_FALSE);
    int c_shape_len = jenv -> GetArrayLength(jshape);

    //Generate pytorch shape
    std::vector<int64_t> torch_shape;
    int i = 0;
    while(i < c_shape_len) {
        torch_shape.push_back(*(c_shape + i));
        i++;
    }
     // create a Tensor
     auto torch_input_tensor = torch::from_blob(c_storage + joffset, torch_shape, at::kFloat);

    // Create a vector of inputs.
    std::vector<torch::jit::IValue> inputs;
    inputs.push_back(torch_input_tensor);
//    inputs.push_back(torch::ones({1, 3, 224, 224}));

    // Execute the model and turn its output into a tensor.
    at::Tensor output = model_ptr->forward(inputs).toTensor();

     // Release critical part
     jenv -> ReleasePrimitiveArrayCritical(jstorage, c_storage, 0);
     jenv -> ReleasePrimitiveArrayCritical(jshape, c_shape, 0);

    // Wrap to Zoo JTensor
    jclass jtensor_class = jenv -> FindClass("com/intel/analytics/zoo/pipeline/inference/JTensor");
    jmethodID jtensor_constructor = jenv -> GetMethodID(jtensor_class, "<init>", "([F[I)V");

    auto sizes = output.sizes();

    int result_storage_len = 1;
    float *pytorch_result_storage = output.data<float>();
    int result_shape_len = sizes.size();

    int pytorch_result_shape[result_shape_len];
    int j = 0;
    while (j < result_shape_len) {
        pytorch_result_shape[j] = sizes[j];
        result_storage_len *= sizes[j];
        j++;
    }


    jfloatArray result_storage = jenv -> NewFloatArray(result_storage_len);
    jenv -> SetFloatArrayRegion(result_storage, 0, result_storage_len, pytorch_result_storage);


    jintArray result_shape = jenv -> NewIntArray(result_shape_len);
    jenv -> SetIntArrayRegion(result_shape, 0, result_shape_len, pytorch_result_shape);

    jobject result = jenv -> NewObject(jtensor_class, jtensor_constructor, result_storage,
    result_shape);

    return result;
  }

}
